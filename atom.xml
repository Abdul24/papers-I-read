<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Papers I Read</title>
 <link href="https://shagunsodhani.in/papers-I-read/atom.xml" rel="self"/>
 <link href="https://shagunsodhani.in/papers-I-read/"/>
 <updated>2017-07-02T16:07:30+05:30</updated>
 <id>https://shagunsodhani.in/papers-I-read</id>
 <author>
   <name>Shagun Sodhani</name>
   <email>sshagunsodhani@gmail.com</email>
 </author>

 
 <entry>
   <title>One Model To Learn Them All</title>
   <link href="https://shagunsodhani.in/papers-I-read/One-Model-To-Learn-Them-All"/>
   <updated>2017-07-01T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/One Model To Learn Them All</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The current trend in deep learning is to design, train and fine tune a separate model for each problem.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-philosophy&quot;&gt;Design Philosophy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;The joint representation is to be of variable size.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Different tasks from the same domain share the modality net.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder and decoder use the following computational blocks:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Convolutional Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Attention Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Multihead, dot product based attention mechanism.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Mixture-of-Experts (MoE) Block&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;For further details, refer the &lt;a href=&quot;https://arxiv.org/abs/1706.05137&quot;&gt;original paper&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Encoder&lt;/strong&gt; consists of 6 conv blocks with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;I/O mixer&lt;/strong&gt; consists of an attention block and 2 conv blocks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt; consists of 4 blocks of convolution and attention with a MoE block in the middle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Modality Nets&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Language Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Input is the sequence of tokens ending in a termination token.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;This sequence is mapped to correct dimensionality using a learned embedding.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt; and &lt;strong&gt;Categorical Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Uses residual convolution blocks.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Similar to the exit flow for &lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception Network&lt;/a&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Audio Data&lt;/strong&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ speech corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ImageNet dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;COCO image captioning dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WSJ parsing dataset&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-German translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-English translation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMT English-French translation corpus&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;German-French translation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The experimental section is not very rigorous with many details skipped (would probably be added later).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While MultiModel does not beat the state of the art models, it does outperform some recent models.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them. It was observed that either the performance either went down or remained the same when MoE component was removed. This indicates that mixing different components does help to improve performance over multiple tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does. The paper does not explore possibilities like these.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Two/Too Simple Adaptations of Word2Vec for Syntax Problems</title>
   <link href="https://shagunsodhani.in/papers-I-read/Two-Too-Simple-Adaptations-of-Word2Vec-for-Syntax-Problems"/>
   <updated>2017-06-26T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/Two-Too Simple Adaptations of Word2Vec for Syntax Problems</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In the original Skip-Gram setting, the model predicts the &lt;em&gt;2c&lt;/em&gt; words in the context window (&lt;em&gt;c&lt;/em&gt; is the size of the context window). But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.&lt;/li&gt;
  &lt;li&gt;Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.&lt;/li&gt;
  &lt;li&gt;The paper proposes to use a set of &lt;em&gt;2c&lt;/em&gt; matrices each for a different word in the context window for both Skip-Gram and CBOW models.&lt;/li&gt;
  &lt;li&gt;This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.&lt;/li&gt;
  &lt;li&gt;The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Decomposable Attention Model for Natural Language Inference</title>
   <link href="https://shagunsodhani.in/papers-I-read/A-Decomposable-Attention-Model-for-Natural-Language-Inference"/>
   <updated>2017-06-17T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/A Decomposable Attention Model for Natural Language Inference</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.&lt;/li&gt;
  &lt;li&gt;Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.01933&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given two sentences &lt;strong&gt;a&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;, the model has to predict whether they have an “entailment” relationship, “neutral” relationship or “contradiction” relationship.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;All the words are mapped to their corresponding word vector representation. In subsequent steps, “word” refers to the word vector representation of the actual word.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attend&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;For each word &lt;em&gt;i&lt;/em&gt; in &lt;strong&gt;a&lt;/strong&gt; and &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;, obtain unnormalized attention weights *e(i, j)=F(i)&lt;sup&gt;T&lt;/sup&gt;F(j) where F is a feed-forward neural network.&lt;/li&gt;
      &lt;li&gt;For &lt;em&gt;i&lt;/em&gt;, compute a β&lt;sub&gt;i&lt;/sub&gt; by performing softmax-like normalization of &lt;em&gt;j&lt;/em&gt; using &lt;em&gt;e(i, j)&lt;/em&gt; as the weight and normalizing for all words &lt;em&gt;j&lt;/em&gt; in &lt;strong&gt;b&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;β&lt;sub&gt;i&lt;/sub&gt; captures the subphrase in &lt;strong&gt;b&lt;/strong&gt; that is softly aligned to &lt;em&gt;a&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Similarly compute α&lt;sub&gt;j&lt;/sub&gt; for &lt;em&gt;j&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compare&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Create two set of comparison vectors, one for &lt;strong&gt;a&lt;/strong&gt; and another for &lt;strong&gt;b&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;For &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;1, i&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(i, β&lt;sub&gt;i&lt;/sub&gt;)).&lt;/li&gt;
      &lt;li&gt;Similarly for &lt;strong&gt;b&lt;/strong&gt;, &lt;strong&gt;v&lt;sub&gt;2, j&lt;/sub&gt;&lt;/strong&gt; = G(concatenate(j, α&lt;sub&gt;j&lt;/sub&gt;))&lt;/li&gt;
      &lt;li&gt;G is another feed-forward neural network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aggregate&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Aggregate over the two set of comparison vectors to obtain &lt;strong&gt;v&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt; and &lt;strong&gt;v&lt;sub&gt;2&lt;/sub&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Feed the aggregated results through the final classifier layer.&lt;/li&gt;
      &lt;li&gt;Multi-class cross-entropy loss function.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-complexity&quot;&gt;Computational Complexity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Computationally, the proposed model is asymptotically as good as LSTM with attention.&lt;/li&gt;
  &lt;li&gt;Assuming that dimensionality of word vectors &amp;gt; length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.&lt;/li&gt;
  &lt;li&gt;Further, the model has the advantage of being parallelizable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.&lt;/li&gt;
  &lt;li&gt;Adding intra-sentence attention further improve the test accuracy by 0.5 percent.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation. &lt;a href=&quot;https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs&quot;&gt;Quora Duplicate Question Detection Challenege&lt;/a&gt;  would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>A Fast and Accurate Dependency Parser using Neural Networks</title>
   <link href="https://shagunsodhani.in/papers-I-read/A-Fast-and-Accurate-Dependency-Parser-using-Neural-Networks"/>
   <updated>2017-06-03T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/A Fast and Accurate Dependency Parser using Neural Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.&lt;/li&gt;
  &lt;li&gt;Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;description-of-the-system&quot;&gt;Description of the system&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The system described in the paper uses &lt;a href=&quot;http://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-056-R1-07-027&quot;&gt;&lt;strong&gt;arc-standard&lt;/strong&gt; system&lt;/a&gt; (a greedy, transition-based dependency parsing system).&lt;/li&gt;
  &lt;li&gt;Words, POS tags and arc labels are represented as d dimensional vectors.&lt;/li&gt;
  &lt;li&gt;S&lt;sup&gt;w&lt;/sup&gt;, S&lt;sup&gt;t&lt;/sup&gt;, S&lt;sup&gt;l&lt;/sup&gt; denote the set of words, POS and labels respectively.&lt;/li&gt;
  &lt;li&gt;Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.&lt;/li&gt;
  &lt;li&gt;Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself. These features come from different embeddings and are not related as such.&lt;/li&gt;
  &lt;li&gt;Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).&lt;/li&gt;
  &lt;li&gt;Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.&lt;/li&gt;
  &lt;li&gt;Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.&lt;/li&gt;
  &lt;li&gt;This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.&lt;/li&gt;
  &lt;li&gt;L2-regularization term is also added to the loss.&lt;/li&gt;
  &lt;li&gt;During inference, a greedy decoding strategy is used and transition with the highest score is chosen.&lt;/li&gt;
  &lt;li&gt;The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;English Penn Treebank (PTB)&lt;/li&gt;
      &lt;li&gt;Chinese Penn Treebank (CTB)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two dependency representations used:
    &lt;ul&gt;
      &lt;li&gt;CoNLL Syntactic Dependencies (CD)&lt;/li&gt;
      &lt;li&gt;Stanford Basic Dependencies (SD)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Metrics:
    &lt;ul&gt;
      &lt;li&gt;Unlabeled Attached Scores (UAS)&lt;/li&gt;
      &lt;li&gt;Labeled Attached Scores (LAS)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Benchmarked against:
    &lt;ul&gt;
      &lt;li&gt;Greedy arc-eager parser&lt;/li&gt;
      &lt;li&gt;Greedy arc-standard parser&lt;/li&gt;
      &lt;li&gt;Malt-Parser&lt;/li&gt;
      &lt;li&gt;MSTParser&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;The system proposed in the paper outperforms all other parsers in both speed and accuracy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;analysis&quot;&gt;Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Cube function gives a 0.8-1.2% improvement over tanh.&lt;/li&gt;
  &lt;li&gt;Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.&lt;/li&gt;
  &lt;li&gt;Using POS and labels gives an improvement of 1.7% and 0.4% respectively.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Neural Module Networks</title>
   <link href="https://shagunsodhani.in/papers-I-read/Neural-Module-Networks"/>
   <updated>2017-05-23T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/Neural Module Networks</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For the task of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;Visual Question Answering&lt;/a&gt;, decompose a question into its linguistic substructures and train a neural network module for each substructure.&lt;/li&gt;
  &lt;li&gt;Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.&lt;/li&gt;
  &lt;li&gt;Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.&lt;/li&gt;
  &lt;li&gt;The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.02799&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Questions tend to be compositional.&lt;/li&gt;
  &lt;li&gt;Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.&lt;/li&gt;
  &lt;li&gt;Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-module-network-for-vqa&quot;&gt;Neural Module Network for VQA&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Training samples of form &lt;em&gt;(w, x, y)&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;w&lt;/em&gt; - Natural Language Question&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;x&lt;/em&gt; - Images&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;y&lt;/em&gt; - Answer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model specified by collection of modules &lt;em&gt;{m}&lt;/em&gt; and a network layout predictor &lt;em&gt;P&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Model instantiates a network based on &lt;em&gt;P(w)&lt;/em&gt; and uses that to encode a distribution &lt;em&gt;P(y|w, x, model_params)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;modules&quot;&gt;Modules&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Find: Finds objects of interest.&lt;/li&gt;
  &lt;li&gt;Transform: Shift regions of attention.&lt;/li&gt;
  &lt;li&gt;Combine: Merge two attention maps into a single one.&lt;/li&gt;
  &lt;li&gt;Describe: Map a pair of attention and input image to a distribution over the labels.&lt;/li&gt;
  &lt;li&gt;Measure: Map attention to a distribution over the labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;natural-language-question-to-networks&quot;&gt;Natural Language Question to Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Map question to the layout which specifies the set of modules and connections between them.&lt;/li&gt;
  &lt;li&gt;Assemble the final network using the layout.&lt;/li&gt;
  &lt;li&gt;Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.&lt;/li&gt;
  &lt;li&gt;eg “what is the colour of the truck?” becomes “colour(truck)”&lt;/li&gt;
  &lt;li&gt;The symbolic representation is mapped to a layout:
    &lt;ul&gt;
      &lt;li&gt;All leaves become &lt;em&gt;find&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All internal nodes become &lt;em&gt;transform/combine&lt;/em&gt; module.&lt;/li&gt;
      &lt;li&gt;All root nodes become &lt;em&gt;describe/measure&lt;/em&gt; module.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;answering-natural-language-question&quot;&gt;Answering Natural Language Question&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Final model combines output from a simple LSTM question encoder with the output of the neural module network.&lt;/li&gt;
  &lt;li&gt;This helps in modelling the syntactic and semantic regularities of the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Since some modules are updated more frequently than others, adaptive per weight learning rates are better.&lt;/li&gt;
  &lt;li&gt;The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).&lt;/li&gt;
  &lt;li&gt;Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.&lt;/li&gt;
  &lt;li&gt;Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</title>
   <link href="https://shagunsodhani.in/papers-I-read/Making-the-V-in-VQA-Matter-Elevating-the-Role-of-Image-Understanding-in-Visual-Question-Answering"/>
   <updated>2017-05-14T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question.&lt;/li&gt;
  &lt;li&gt;For example, if the question starts with “Do you see a …”, it is more likely to be “yes” than “no”.&lt;/li&gt;
  &lt;li&gt;To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.&lt;/li&gt;
  &lt;li&gt;The authors present a balanced version of &lt;a href=&quot;https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering&quot;&gt;VQA dataset&lt;/a&gt; where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.&lt;/li&gt;
  &lt;li&gt;The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.00837&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset-collection&quot;&gt;Dataset Collection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I’ which is similar to I but for which the answer to question Q is A’ (different from A).&lt;/li&gt;
  &lt;li&gt;To facilitate the search for I’, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A. In case none of the 24 images qualifies, the worker may select “not possible”.&lt;/li&gt;
  &lt;li&gt;In the second round, the workers were asked to answer Q for I’.&lt;/li&gt;
  &lt;li&gt;This 2-stage protocol results in a significantly more balanced dataset than the previous dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;observation&quot;&gt;Observation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.&lt;/li&gt;
  &lt;li&gt;Training on balanced dataset improves performance on the unbalanced dataset.&lt;/li&gt;
  &lt;li&gt;Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;counter-example-explanations&quot;&gt;Counter-example Explanations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.&lt;/li&gt;
  &lt;li&gt;Supervising signal is provided by the data collection procedure where humans pick the image I’ from the same set of candidate images.&lt;/li&gt;
  &lt;li&gt;For each image in the candidate set, compute the inner product of question-image embedding and answer embedding.&lt;/li&gt;
  &lt;li&gt;The K inner product values are passed through a fully connected layer to generate K scores.&lt;/li&gt;
  &lt;li&gt;Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).&lt;/li&gt;
  &lt;li&gt;The proposed explanation model achieves a recall@5 of 43.49%&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Conditional Similarity Networks</title>
   <link href="https://shagunsodhani.in/papers-I-read/Conditional-Similarity-Networks"/>
   <updated>2017-05-07T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/Conditional Similarity Networks</id>
   <content type="html">&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.&lt;/li&gt;
  &lt;li&gt;But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.&lt;/li&gt;
  &lt;li&gt;What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.&lt;/li&gt;
  &lt;li&gt;The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.&lt;/li&gt;
  &lt;li&gt;It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vision.cornell.edu/se3/conditional-similarity-networks/&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conditional-similarity-networks&quot;&gt;Conditional Similarity Networks&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Given an image, &lt;em&gt;x&lt;/em&gt;, learn a non-linear feature embedding &lt;em&gt;f(x)&lt;/em&gt; such that for any 2 images &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt;, the euclidean distance between &lt;em&gt;f(x&lt;sub&gt;1&lt;/sub&gt;)&lt;/em&gt; and &lt;em&gt;f(x&lt;sub&gt;2&lt;/sub&gt;)&lt;/em&gt; reflects their similarity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conditional-similarity-triplets&quot;&gt;Conditional Similarity Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Given a triplet of images &lt;em&gt;(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;)&lt;/em&gt; and a condition &lt;em&gt;c&lt;/em&gt; (the notion of similarity), an oracle (say crowd) is used to determmine if &lt;em&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/em&gt; is more similar to &lt;em&gt;x&lt;sub&gt;2&lt;/sub&gt;&lt;/em&gt; or &lt;em&gt;x&lt;sub&gt;3&lt;/sub&gt;&lt;/em&gt; as per the given criteria &lt;em&gt;c&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;In general, for images &lt;em&gt;i, j, l&lt;/em&gt;, the triplet &lt;em&gt;t&lt;/em&gt; is ordered {i, j, l | c} if &lt;em&gt;i&lt;/em&gt; is more similar to &lt;em&gt;j&lt;/em&gt; than &lt;em&gt;l&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-from-triplets&quot;&gt;Learning From Triplets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Define a loss function &lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;()&lt;/em&gt; to model the similarity structure over the triplets.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;L&lt;sub&gt;T&lt;/sub&gt;(i, j, l) = max{0, D(i, j) - D(i, l) + h}&lt;/em&gt; where &lt;em&gt;D&lt;/em&gt; is the euclidean distance function and &lt;em&gt;h&lt;/em&gt; is the similarity scalar margin to prevent trivial solutions.&lt;/li&gt;
  &lt;li&gt;To model conditional similarities, masks &lt;em&gt;m&lt;/em&gt; are defined as &lt;em&gt;m = σ(β)&lt;/em&gt; where σ is the RELU unit and β is a set of parameters to be learnt.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt; denotes the selection of the c-th mask column from feature vector. It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.&lt;/li&gt;
  &lt;li&gt;The euclidean function &lt;em&gt;D&lt;/em&gt; now computes the masked distance (&lt;em&gt;f(i, c)m&lt;sub&gt;c&lt;/sub&gt;&lt;/em&gt;) between the two given images.&lt;/li&gt;
  &lt;li&gt;Two regularising terms are also added - L2 norm for &lt;em&gt;D&lt;/em&gt; and L1 norm for &lt;em&gt;m&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fonts dataset by Bernhardsson
    &lt;ul&gt;
      &lt;li&gt;3.1 million 64 by 64-pixel grey scale images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zappos50k shoe dataset
    &lt;ul&gt;
      &lt;li&gt;Contains 50,000 images of individual richly annotated shoes.&lt;/li&gt;
      &lt;li&gt;Characteristics of interest:
        &lt;ul&gt;
          &lt;li&gt;Type of the shoes (i.e., shoes, boots, sandals or slippers)&lt;/li&gt;
          &lt;li&gt;Suggested gender of the shoes (i.e., for women, men, girls or boys)&lt;/li&gt;
          &lt;li&gt;Height of the shoes’ heels (0 to 5 inches)&lt;/li&gt;
          &lt;li&gt;Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Initial model for the experiments is a ConvNet pre-trained on ImageNet&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Standard Triplet Network&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learn from all available triplets jointly as if they have the same notion of similarity.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Set of Task Specific Triplet Networks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Train n separate triplet networks such that each is trained on a single notion of similarity.&lt;/li&gt;
      &lt;li&gt;Needs far more parameters and compute.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - fixed disjoint masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.&lt;/li&gt;
      &lt;li&gt;Aims to learn a fully disjoint embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Conditional Similarity Networks - learned masks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Learns all the components - conv filters, embedding and the masks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Refer paper for details on hyperparameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.&lt;/li&gt;
  &lt;li&gt;The learned masks are very sparse and share dimensions. This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.&lt;/li&gt;
  &lt;li&gt;Order of performance:
    &lt;ul&gt;
      &lt;li&gt;CSNs with learned masks &amp;gt; CSNs with fixed masks &amp;gt; Task-specific networks &amp;gt; standard triplet network.&lt;/li&gt;
      &lt;li&gt;Though CSNs with learned masks require more training data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.&lt;/li&gt;
  &lt;li&gt;This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple Baseline for Visual Question Answering</title>
   <link href="https://shagunsodhani.in/papers-I-read/Simple-Baseline-for-Visual-Question-Answering"/>
   <updated>2017-04-28T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/Simple Baseline for Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/li&gt;
  &lt;li&gt;The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1512.02167.pdf&quot;&gt;Link to the paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Text Features&lt;/strong&gt; - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Features&lt;/strong&gt; - Last layer activations from GoogLeNet.&lt;/li&gt;
  &lt;li&gt;Text features are concatenated with image features and fed into a softmax.&lt;/li&gt;
  &lt;li&gt;Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interpretation-of-the-model&quot;&gt;Interpretation of the model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning. This is the greatest strength of the paper even though the model is very simple and naive.&lt;/li&gt;
  &lt;li&gt;The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.&lt;/li&gt;
  &lt;li&gt;Question words generally can influence the answer given the bias in images occurring in COCO dataset.&lt;/li&gt;
  &lt;li&gt;Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.&lt;/li&gt;
  &lt;li&gt;The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.&lt;/li&gt;
  &lt;li&gt;While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>VQA-Visual Question Answering</title>
   <link href="https://shagunsodhani.in/papers-I-read/VQA-Visual-Question-Answering"/>
   <updated>2017-04-27T00:00:00+05:30</updated>
   <id>https://shagunsodhani.in/papers-I-read/VQA Visual Question Answering</id>
   <content type="html">&lt;h3 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1505.00468v6&quot;&gt;Link to the paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vqa-challenge-and-workshop&quot;&gt;&lt;a href=&quot;http://www.visualqa.org/&quot;&gt;VQA Challenge and Workshop&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.&lt;/li&gt;
  &lt;li&gt;Interestingly, the second version is starting on 27th April 2017 (today).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;benefits-over-tasks-like-image-captioning&quot;&gt;Benefits over tasks like image captioning:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Simple, &lt;em&gt;n-gram&lt;/em&gt; statistics based methods are not sufficient.&lt;/li&gt;
  &lt;li&gt;Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.&lt;/li&gt;
  &lt;li&gt;Since only short answers are expected, evaluation is easier.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Created a new dataset of 50000 realistic, abstract images.&lt;/li&gt;
  &lt;li&gt;Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (&amp;gt;200K images) and abstract images.&lt;/li&gt;
  &lt;li&gt;Three questions per image and ten answers per question (along with their confidence) were collected.&lt;/li&gt;
  &lt;li&gt;The entire dataset contains over 760K questions and 10M answers.&lt;/li&gt;
  &lt;li&gt;The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-of-data-collection-methodology&quot;&gt;Highlights of data collection methodology&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Emphasis on questions that require an image, and not just common sense, to be answered correctly.&lt;/li&gt;
  &lt;li&gt;Workers were shown previous questions when writing new questions to increase diversity.&lt;/li&gt;
  &lt;li&gt;Answers collected from multiple users to account for discrepancies in answers by humans.&lt;/li&gt;
  &lt;li&gt;Two modalities supported:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Open-ended&lt;/strong&gt; - produce the answer&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;multiple-choice&lt;/strong&gt; - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;highlights-from-data-analysis&quot;&gt;Highlights from data analysis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Most questions range from four to ten words while answers range from one to three words.&lt;/li&gt;
  &lt;li&gt;Around 40% questions are “yes/no” questions.&lt;/li&gt;
  &lt;li&gt;Significant (&amp;gt;80%) inter-human agreement for answers.&lt;/li&gt;
  &lt;li&gt;The authors performed a study where human evaluators were asked to answer the questions without looking at the images.&lt;/li&gt;
  &lt;li&gt;Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.&lt;/li&gt;
  &lt;li&gt;The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baseline-models&quot;&gt;Baseline Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;random&lt;/strong&gt; selection&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;prior (“yes”)&lt;/strong&gt; - always answer as yes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;per Q-type prior&lt;/strong&gt; - pick the most popular answer per question type.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;nearest neighbor&lt;/strong&gt; - find the k nearest neighbors for the given (image, question) pair.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Image Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;I&lt;/strong&gt; - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;norm I&lt;/strong&gt; - : l2 normalized version of &lt;strong&gt;I&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Question Channel&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q&lt;/strong&gt; - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q&lt;/strong&gt; - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity. These embeddings are fed to an LSTM to obtain 1024d-dim embedding.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Deeper LSTM Q&lt;/strong&gt; - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-Layer Perceptron (MLP)&lt;/strong&gt; - Combine image and question embeddings to obtain a single embedding.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BoW Q + I&lt;/strong&gt; method - concatenate BoW Q and I embeddings.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LSTM Q + I, deeper LSTM Q + norm I&lt;/strong&gt; methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.&lt;/li&gt;
  &lt;li&gt;Cross-entropy loss with VGGNet parameters frozen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (&amp;gt;80% and &amp;gt;90% respectively).&lt;/li&gt;
  &lt;li&gt;The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.&lt;/li&gt;
  &lt;li&gt;Vision only model performs even worse than the model which always produces “yes” as the answer.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
